K-Nearest Neighbors (KNN) Classification ‚Äì README.md
üìå Objective

Learn and implement the K-Nearest Neighbors (KNN) algorithm for classification using the Iris dataset.

üìÇ Dataset

Using the uploaded dataset:

/mnt/data/Iris.csv


The dataset contains the following features:

SepalLengthCm

SepalWidthCm

PetalLengthCm

PetalWidthCm

Species (Target)

üõ†Ô∏è Tools Used

Python

Pandas

Matplotlib

Scikit-learn

StandardScaler

üöÄ Project Steps
1. Load and Prepare the Dataset

Load CSV file

Drop unnecessary columns (like Id)

Separate features (X) and target (y)

2. Normalize the Features

KNN is distance-based; hence scaling is crucial.

3. Train-Test Split

Use 80% for training, 20% for testing.

4. Train KNN Model

Use KNeighborsClassifier from sklearn.

5. Test and Evaluate

Accuracy score

Confusion matrix visualization

6. Experiment with K

You can try K = 1, 3, 5, 7, 9 and see accuracy changes.

üß™ KNN Code (Beginner-Friendly)
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv("/mnt/data/Iris.csv")

# Drop ID column if exists
df = df.drop(columns=["Id"], errors="ignore")

# Features and Target
X = df.drop("Species", axis=1)
y = df["Species"]

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# KNN Model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Prediction
y_pred = knn.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

üéØ What You Learn

How KNN works

Why normalization is important

How to select the best K

Importance of distance metrics

How KNN handles multi-class classification

‚ùì Interview Questions
1. How does KNN algorithm work?

It finds the K nearest neighbors and predicts the majority class among them.

2. How do you choose the right K?

Use:

Odd values

Cross-validation

Plot K vs Accuracy

3. Why is normalization important in KNN?

Because distance metrics get biased if feature scales differ.

4. Time complexity of KNN?

Training: O(1)

Prediction: O(n √ó d) (slow on large datasets)

5. Pros & Cons of KNN?

Pros: Simple, no training, flexible
Cons: Slow prediction, sensitive to noise, needs scaling

6. Is KNN sensitive to noise?

Yes, noisy points affect nearest neighbors.

7. How does KNN handle multi-class?

Majority voting among K neighbors automatically works for multi-class datasets.

8. Role of distance metrics?

Defines how ‚Äúcloseness‚Äù is measured:

Euclidean (default)

Manhattan

Minkowski