# ğŸ§  Task 5 â€” Decision Trees & Random Forests  
### Machine Learning Mini Project

## ğŸ“Œ Objective  
Learn how tree-based models work for both **classification** and **regression**, and understand concepts like entropy, information gain, overfitting, bagging, and feature importance.

---

## ğŸ“š Dataset  
We use the **Heart Disease Dataset** (`heart.csv`).

Upload path in environment:  
`/mnt/data/heart.csv`

---

## ğŸ›  Tools & Libraries  
- Python  
- Scikit-learn  
- Pandas  
- Matplotlib / Seaborn  
- Graphviz (optional for tree visualization)

---

## ğŸš€ Step-by-Step Implementation

### **1ï¸âƒ£ Import Libraries & Load Dataset**
```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv("/mnt/data/heart.csv")
X = df.drop("target", axis=1)
y = df["target"]
2ï¸âƒ£ Trainâ€“Test Split
python
Copy code
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
3ï¸âƒ£ Decision Tree Model
python
Copy code
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

dt_pred = dt.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))
4ï¸âƒ£ Random Forest Model
python
Copy code
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

rf_pred = rf.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, rf_pred))
5ï¸âƒ£ Feature Importance
python
Copy code
importances = rf.feature_importances_
for feature, score in zip(X.columns, importances):
    print(f"{feature}: {score:.4f}")
ğŸ“Š Visualization (Optional)
Decision Tree Plot
python
Copy code
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(14,7))
plot_tree(dt, feature_names=X.columns, filled=True)
plt.show()
ğŸ“˜ What You Learn
âœ” How Decision Trees work
âœ” Entropy & Information Gain
âœ” Random Forest and Bagging
âœ” Train/Test split and evaluation
âœ” Overfitting vs Generalization
âœ” Feature Importance Interpretation

ğŸ§ª Interview Questions & Answers
1. How does a decision tree work?
It splits the data by asking ifâ€“else questions to make pure groups.

2. What is entropy and information gain?
Entropy â†’ impurity

Information gain â†’ reduction in impurity after a split

3. How is Random Forest better than a single tree?
Uses multiple trees + voting â†’ less overfitting and higher accuracy.

4. What is overfitting? How do you prevent it?
When model learns noise. Prevent using:

max_depth

pruning

random forest

cross-validation

5. What is bagging?
Training multiple models on bootstrapped samples and averaging output.

6. How to visualize a decision tree?
Using plot_tree() or Graphviz.

7. What is feature importance?
Scores showing how much each feature helps in splitting the data.

8. Pros/Cons of Random Forest
Pros: high accuracy, less overfitting, robust
Cons: slow, large model, less interpretable

ğŸ“¦ Project Files
decision_tree.png â€” tree visualization

feature_importances.csv

cm_random_forest.png

depth_analysis.png

(All generated by model)

âœ… Conclusion
This project demonstrates how Decision Trees and Random Forests can be used for classification tasks.
You have learned:

How to build and evaluate models

How to handle overfitting

How ensemble methods improve accuracy

